import { NextResponse } from 'next/server';
import axios from 'axios';

// Processing rules helper function
function applyPreprocessingRules(content, rules) {
  if (!content || !rules) return content;
  
  let processedContent = content;
  
  // Remove sensitive data (basic implementation)
  if (rules.removeSensitiveData) {
    // Remove email patterns
    processedContent = processedContent.replace(/[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g, '[EMAIL_REMOVED]');
    // Remove phone patterns
    processedContent = processedContent.replace(/(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}/g, '[PHONE_REMOVED]');
    // Remove SSN-like patterns
    processedContent = processedContent.replace(/\d{3}-\d{2}-\d{4}/g, '[SSN_REMOVED]');
  }
  
  // Remove hyperlinks
  if (rules.removeHyperlinks) {
    // Remove URLs
    processedContent = processedContent.replace(/https?:\/\/[^\s]+/g, '[LINK_REMOVED]');
    // Remove markdown links
    processedContent = processedContent.replace(/\[([^\]]+)\]\([^\)]+\)/g, '$1');
  }
  
  // Format text (basic implementation)
  if (rules.formatText) {
    // Clean up extra whitespace
    processedContent = processedContent.replace(/\s+/g, ' ').trim();
    // Fix common punctuation issues
    processedContent = processedContent.replace(/\s+([.!?])/g, '$1');
  }
  
  return processedContent;
}

// Post-processing rules helper function
function applyPostprocessingRules(content, rules) {
  if (!content || !rules) return content;
  
  let processedContent = content;
  
  // Add citations (basic implementation)
  if (rules.addCitations) {
    // Add a simple citation note at the end
    if (!processedContent.includes('[Citation:')) {
      processedContent += '\n\n[Citation: Response generated by BELTO AI educational assistant]';
    }
  }
  
  return processedContent;
}

// Optimized endpoint array with health check approach (no request timeouts)
const endpoints = [
  {
    url: 'http://bel2ai.duckdns.org:8001/v1/chat/completions',
    name: 'Llama 3.1 8B Instruct (RTX 3090)',
    model: 'Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf',
    type: 'chat',
    context: 16384,
    parallel: 6,
    priority: 1, // Highest priority - best context size and parallel processing
    healthCheckTimeout: 3000 // Quick health check timeout
  },
  {
    url: 'http://bel2ai.duckdns.org:8002/v1/completions',
    name: 'GPT-OSS 20B (RTX 3090)',
    model: 'gpt-oss-20b.Q8_0.gguf',
    type: 'completion',
    context: 8192,
    parallel: 3,
    priority: 2, // Second priority - good performance, fastest response time
    healthCheckTimeout: 3000
  },
  {
    url: 'http://bigbelto.duckdns.org:8004/v1/completions',
    name: 'GPT-OSS 20B F16 (RTX 4090)',
    model: 'gpt-oss-20b-F16.gguf',
    type: 'completion',
    context: 8192,
    slots: 1,
    priority: 3, // Third priority - powerful but single slot
    healthCheckTimeout: 3000
  },
  {
    url: 'http://bigbelto.duckdns.org:8005/v1/completions',
    name: 'Hermes-3 Llama-3.2-3B (RTX 4090)',
    model: 'Hermes-3-Llama-3.2-3B.Q8_0.gguf',
    type: 'completion',
    context: 4096,
    slots: 1,
    priority: 4, // Lower priority - smaller context
    healthCheckTimeout: 3000
  },
  {
    url: 'http://minibelto.duckdns.org:8007/v1/completions',
    name: 'DeepSeek 7B Chat (RTX 3060 Ti)',
    model: 'deepseek-llm-7b-chat.Q4_K_M.gguf',
    type: 'completion',
    context: 8192,
    slots: 1,
    priority: 5, // Medium priority - good context size
    healthCheckTimeout: 3000
  },
  {
    url: 'http://doublebelto.duckdns.org:8008/v1/completions',
    name: 'GPT-OSS 20B Q4 (Double RTX 3060)',
    model: 'gpt-oss-20b-Q4_K_M.gguf',
    type: 'completion',
    context: 4096,
    slots: 1,
    priority: 6, // Lower priority - smaller context
    healthCheckTimeout: 3000
  },
  {
    url: 'http://doublebelto.duckdns.org:8009/v1/completions',
    name: 'Hermes-3B Q4 (Double RTX 3060)',
    model: 'Hermes-3-Llama-3.2-3B-Q4_K_M.gguf',
    type: 'completion',
    context: 4096,
    slots: 1,
    priority: 7, // Lowest priority - smallest model
    healthCheckTimeout: 3000
  }
];

// Add a flag to enable fallback responses when all endpoints fail - RE-ENABLED with better conditions
const ENABLE_FALLBACK_RESPONSES = true;

// 'http://97.90.195.162:9999/v1/chat/completions',

// Enhanced endpoint health tracking with priority-based selection
const endpointStats = endpoints.map(endpoint => ({
  url: endpoint.url,
  name: endpoint.name,
  model: endpoint.model,
  type: endpoint.type,
  context: endpoint.context,
  priority: endpoint.priority,
  timeout: endpoint.timeout,
  isAvailable: true,
  failCount: 0,
  lastResponseTime: 0,
  lastChecked: Date.now(),
  consecutiveFailures: 0,
  circuitBreakerOpen: false,
  lastCircuitBreakerCheck: Date.now(),
  successRate: 1.0, // Track success rate for intelligent selection
  avgResponseTime: 0 // Track average response time
}));

// Optimized timeouts for faster, complete responses
const ULTRA_FAST_TIMEOUT_MS = 4000; // Increased from 3000 for more reliable responses
const FAST_TIMEOUT_MS = 6000; // Increased from 5000 for better completion rates
const BASE_TIMEOUT_MS = 10000; // Increased from 8000 for complete responses
const ATTACHMENT_TIMEOUT_MS = 25000; // Increased from 20000 for document processing
const MAX_CONSECUTIVE_FAILURES = 2; // Keep balanced failure threshold
const RETRY_INTERVAL_MS = 30000; // Increased to 30 seconds for better recovery
const HEALTH_CHECK_THRESHOLD = 180000; // Increased to 3 minutes for stability
const CIRCUIT_BREAKER_THRESHOLD = 3; // Keep balanced circuit breaker
const CIRCUIT_BREAKER_TIMEOUT = 60000; // Increased to 60 seconds for better recovery

/**
 * Determines appropriate timeout based on request complexity - OPTIMIZED FOR SPEED
 * @param {Object} body - Request body
 * @param {Array} messages - Formatted messages array
 * @returns {number} Timeout in milliseconds
 */
function getTimeoutForRequest(body, messages) {
  // Check for attachments or large content
  const hasAttachments = body.attachments && body.attachments.length > 0;
  const hasLargeContent = messages.some(msg => msg.content && msg.content.length > 1000);
  const totalContentLength = messages.reduce((sum, msg) => sum + (msg.content?.length || 0), 0);
  
  // ULTRA-FAST TRACK: Match curl performance for simple messages
  if (!hasAttachments && !hasLargeContent && totalContentLength < 300) {
    console.log(`üöÄ Using ULTRA-FAST timeout (${ULTRA_FAST_TIMEOUT_MS}ms) for simple message: ${totalContentLength} chars`);
    return ULTRA_FAST_TIMEOUT_MS;
  }
  // FAST TRACK: Quick processing for short messages
  if (!hasAttachments && !hasLargeContent && totalContentLength < 800) {
    console.log(`‚ö° Using FAST timeout (${FAST_TIMEOUT_MS}ms) for short message: ${totalContentLength} chars`);
    return FAST_TIMEOUT_MS;
  }
  // ADAPTIVE: For document/large requests, scale timeout with size
  if (hasAttachments) {
    const docSize = body.attachments.reduce((max, att) => Math.max(max, att.content?.length || 0), 0);
    if (docSize > 100000) {
      console.log(`üìÑ Large document detected (${docSize} chars), using 60s timeout`);
      return 60000;
    } else if (docSize > 50000) {
      console.log(`üìÑ Medium-large document detected (${docSize} chars), using 45s timeout`);
      return 45000;
    } else if (docSize > 20000) {
      console.log(`üìÑ Medium document detected (${docSize} chars), using 30s timeout`);
      return 30000;
    } else {
      console.log(`üìÑ Small document detected (${docSize} chars), using 20s timeout`);
      return 20000;
    }
  }
  
  // Use processing hints for smarter timeout calculation (only for complex requests)
  if (body.processingHints && hasAttachments) {
    const hints = body.processingHints;
    console.log(`üïê Calculating timeout using processing hints:`, hints);
    
    // PDF documents typically need more processing time
    if (hints.documentType === 'pdf') {
      if (hints.contentLength > 20000) {
        console.log(`Using maximum timeout (45000ms) for large PDF document: ${hints.contentLength} chars`);
        return 45000; // 45 seconds for large PDFs
      } else if (hints.contentLength > 10000) {
        console.log(`Using extended timeout (35000ms) for medium PDF document: ${hints.contentLength} chars`);
        return 35000; // 35 seconds for medium PDFs
      } else {
        console.log(`Using enhanced timeout (25000ms) for small PDF document: ${hints.contentLength} chars`);
        return 25000; // 25 seconds for small PDFs
      }
    }
    
    // DOC files processing
    if (hints.documentType === 'doc' || hints.documentType === 'docx') {
      if (hints.contentLength > 15000) {
        console.log(`Using enhanced timeout (35000ms) for large DOC document: ${hints.contentLength} chars`);
        return 35000;
      } else {
        console.log(`Using enhanced timeout (25000ms) for DOC document: ${hints.contentLength} chars`);
        return 25000;
      }
    }
    
    // Analysis vs Summary - Analysis needs more time
    if (hints.analysisType === 'analysis' && hints.contentLength > 5000) {
      console.log(`Using analysis timeout (30000ms) for detailed analysis: ${hints.contentLength} chars`);
      return 30000;
    }
  }
  
  // Check for code-related requests (only if no fast track)
  const hasCodeKeywords = messages.some(msg => 
    msg.content && /\b(code|function|class|variable|algorithm|program|java|python|javascript|html|css)\b/i.test(msg.content)
  );
  
  // Fallback to original logic for non-hinted requests
  if (hasAttachments) {
    const hasPDFContent = body.attachments.some(att => 
      att.content && att.content.length > 5000 || 
      att.name && att.name.toLowerCase().includes('.pdf')
    );
    
    if (hasPDFContent || totalContentLength > 10000) {
      console.log(`Using maximum timeout (45000ms) for large PDF/document request:`, {
        hasAttachments,
        totalContentLength,
        attachmentSizes: body.attachments.map(att => att.content?.length || 0)
      });
      return 45000; // 45 seconds for large PDFs
    }
  }
  
  if (hasAttachments || hasLargeContent || totalContentLength > 2000 || hasCodeKeywords) {
    console.log(`Using extended timeout (${ATTACHMENT_TIMEOUT_MS}ms) for complex request:`, {
      hasAttachments,
      hasLargeContent,
      totalContentLength,
      hasCodeKeywords
    });
    return ATTACHMENT_TIMEOUT_MS;
  }
  
  console.log(`Using base timeout (${BASE_TIMEOUT_MS}ms) for normal request`);
  return BASE_TIMEOUT_MS;
}

/**
 * Quick health check for an endpoint to verify availability
 * @param {Object} endpointConfig - The endpoint configuration
 * @returns {Promise<boolean>} True if endpoint is healthy
 */
async function quickHealthCheck(endpointConfig) {
  try {
    console.log(`üîç Health checking ${endpointConfig.name}...`);
    
    // Create a minimal test payload
    let testPayload;
    if (endpointConfig.type === 'chat') {
      testPayload = {
        model: 'local',
        messages: [
          { role: 'user', content: 'test' }
        ],
        temperature: 0.1,
        max_tokens: 5 // Minimal response for health check
      };
    } else {
      testPayload = {
        model: 'local',
        prompt: 'User: test\nBELTO AI:',
        temperature: 0.1,
        max_tokens: 5 // Minimal response for health check
      };
    }
    
    const startTime = Date.now();
    const response = await axios.post(endpointConfig.url, testPayload, {
      headers: { 'Content-Type': 'application/json' },
      timeout: endpointConfig.healthCheckTimeout,
      validateStatus: function (status) {
        return status >= 200 && status < 500;
      }
    });
    
    const responseTime = Date.now() - startTime;
    const isHealthy = response.status === 200;
    
    if (isHealthy) {
      console.log(`‚úÖ ${endpointConfig.name} is healthy (${responseTime}ms)`);
      // Update endpoint stats with health check success
      updateEndpointStats(endpointConfig.url, true, responseTime);
    } else {
      console.log(`‚ö†Ô∏è ${endpointConfig.name} returned ${response.status}`);
      updateEndpointStats(endpointConfig.url, false, 0);
    }
    
    return isHealthy;
    
  } catch (error) {
    console.log(`‚ùå ${endpointConfig.name} health check failed: ${error.message}`);
    updateEndpointStats(endpointConfig.url, false, 0);
    return false;
  }
}

/**
 * Find the best available endpoint by running quick health checks
 * @param {Array} preferredEndpoints - Array of endpoint URLs in preference order (optional)
 * @returns {Promise<Object|null>} Endpoint config of the first healthy endpoint or null
 */
async function findHealthyEndpoint(preferredEndpoints = null) {
  // If no preferred endpoints provided, use all endpoints sorted by priority
  const endpointsToCheck = preferredEndpoints || endpoints
    .sort((a, b) => a.priority - b.priority)
    .map(e => e.url);
  
  console.log(`üéØ Checking health of ${endpointsToCheck.length} endpoints...`);
  
  for (const endpointUrl of endpointsToCheck) {
    const endpointConfig = endpoints.find(e => e.url === endpointUrl);
    if (!endpointConfig) continue;
    
    // Skip if circuit breaker is open
    const endpointStat = endpointStats.find(e => e.url === endpointUrl);
    if (endpointStat && endpointStat.circuitBreakerOpen) {
      console.log(`‚ö° Skipping ${endpointConfig.name} (circuit breaker open)`);
      continue;
    }
    
    // Perform quick health check
    const isHealthy = await quickHealthCheck(endpointConfig);
    if (isHealthy) {
      console.log(`üéØ Selected healthy endpoint: ${endpointConfig.name}`);
      return endpointConfig; // Return the full endpoint config instead of just URL
    }
  }
  
  console.log('‚ö†Ô∏è No healthy endpoints found');
  return null;
}

/**
 * Enhanced endpoint statistics tracking with performance metrics
 * @param {string} url - The endpoint URL
 * @param {boolean} success - Whether the request was successful
 * @param {number} responseTime - Response time in milliseconds
 */
function updateEndpointStats(url, success, responseTime) {
  const endpoint = endpointStats.find(e => e.url === url);
  if (!endpoint) return;
  
  endpoint.lastChecked = Date.now();
  
  if (success) {
    endpoint.isAvailable = true;
    endpoint.lastResponseTime = responseTime;
    endpoint.consecutiveFailures = 0;
    endpoint.circuitBreakerOpen = false;
    
    // Update average response time (rolling average)
    if (endpoint.avgResponseTime === 0) {
      endpoint.avgResponseTime = responseTime;
    } else {
      endpoint.avgResponseTime = (endpoint.avgResponseTime * 0.7) + (responseTime * 0.3);
    }
    
    // Update success rate (rolling success rate)
    endpoint.successRate = Math.min(1.0, endpoint.successRate * 0.9 + 0.1);
    
    // Gradually reduce fail count on success
    if (endpoint.failCount > 0) {
      endpoint.failCount = Math.max(0, endpoint.failCount - 1);
    }
    
    console.log(`‚úÖ ${endpoint.name}: ${responseTime}ms (Avg: ${endpoint.avgResponseTime.toFixed(0)}ms, Success: ${(endpoint.successRate * 100).toFixed(1)}%)`);
  } else {
    endpoint.failCount++;
    endpoint.consecutiveFailures++;
    
    // Update success rate (rolling failure impact)
    endpoint.successRate = Math.max(0.0, endpoint.successRate * 0.8);
    
    // Check if we should open the circuit breaker
    if (endpoint.consecutiveFailures >= CIRCUIT_BREAKER_THRESHOLD) {
      console.log(`üî¥ Opening circuit breaker for ${endpoint.name} after ${endpoint.consecutiveFailures} consecutive failures`);
      endpoint.circuitBreakerOpen = true;
      endpoint.lastCircuitBreakerCheck = Date.now();
    }
    
    if (endpoint.consecutiveFailures >= MAX_CONSECUTIVE_FAILURES) {
      console.log(`‚ö†Ô∏è Marking ${endpoint.name} as unavailable after ${endpoint.consecutiveFailures} consecutive failures`);
      endpoint.isAvailable = false;
    }
    
    console.log(`‚ùå ${endpoint.name}: Failed (Success: ${(endpoint.successRate * 100).toFixed(1)}%, Failures: ${endpoint.consecutiveFailures})`);
  }
}

/**
 * Performs a health check on all endpoints
 */
async function healthCheck() {
  console.log('üîç Performing health check on all endpoints');
  
  const checks = endpoints.map(async (endpointConfig) => {
    const endpoint = endpointStats.find(e => e.url === endpointConfig.url);
    if (!endpoint) return;
    
    try {
      const startTime = Date.now();
      // Use endpoint-specific format for health check
      const testMessages = [{ role: 'user', content: 'test' }];
      const requestConfig = formatRequestForEndpoint(endpointConfig.url, testMessages, process.env.AI_API_KEY || 'test');
      
      const response = await axios.post(requestConfig.url, requestConfig.data, {
        timeout: 8000, // Increased timeout for health check
        headers: requestConfig.headers
      });
      
      const responseTime = Date.now() - startTime;
      updateEndpointStats(endpointConfig.url, true, responseTime);
      
      // Parse response to get content for logging
      const parsedResponse = parseResponseFromEndpoint(response, endpointConfig.url);
      console.log(`‚úÖ Health check for ${endpointConfig.name}: OK (${responseTime}ms)`);
      console.log(`   Response: ${parsedResponse.content?.substring(0, 50) || 'No content'}...`);
    } catch (error) {
      console.log(`‚ùå Health check for ${endpointConfig.name}: FAILED`);
      console.log(`   Error: ${error.code || error.message}`);
      console.log(`   Status: ${error.response?.status || 'No response'}`);
      console.log(`   Data: ${JSON.stringify(error.response?.data || {})}`);
      // Don't mark as failed during health check to avoid being too aggressive
      // updateEndpointStats(endpointConfig.url, false, 0);
    }
  });
  
  await Promise.allSettled(checks);
}

// Periodically check endpoints health
let healthCheckInterval = null;

// Initialize health check with immediate execution
const initializeHealthCheck = () => {
  // Run initial health check
  setTimeout(() => {
    healthCheck();
  }, 5000); // Wait 5 seconds after startup
  
  // Set up periodic health checks
  healthCheckInterval = setInterval(() => {
    const now = Date.now();
    // Only perform health check if enough time has passed since the last check
    const needsCheck = endpointStats.some(
      endpoint => now - endpoint.lastChecked > HEALTH_CHECK_THRESHOLD
    );
    
    if (needsCheck) {
      healthCheck();
    }
  }, HEALTH_CHECK_THRESHOLD / 2); // Check twice as often as the threshold
};

/**
 * Enhanced request formatting for different endpoint types with no timeout limits
 * @param {string} endpoint - The endpoint URL  
 * @param {Array} messages - The messages array
 * @param {string} apiKey - The API key
 * @returns {Object} Formatted request config
 */
function formatRequestForEndpoint(endpoint, messages, apiKey) {
  const endpointConfig = endpoints.find(e => e.url === endpoint);
  
  if (!endpointConfig) {
    throw new Error(`Unknown endpoint: ${endpoint}`);
  }

  if (endpointConfig.type === 'chat') {
    // Chat completions format (only port 8001 - Llama 3.1 8B)
    return {
      url: endpoint,
      data: {
        model: 'local',
        messages: messages,
        // Optimized settings for complete responses - NO TIMEOUT
        temperature: 0.7,
        top_p: 0.9,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        // Minimal stop sequences to prevent cutting off responses
        stop: []
      },
      headers: {
        'Content-Type': 'application/json'
      }
    };
  } else {
    // Completions format (all other ports)
    const prompt = messages.map(msg => {
      if (msg.role === 'system') {
        return `System: ${msg.content}`;
      }
      if (msg.role === 'user') return `User: ${msg.content}`;
      if (msg.role === 'assistant') return `BELTO AI: ${msg.content}`;
      return msg.content;
    }).join('\n') + '\nBELTO AI:';
    
    return {
      url: endpoint,
      data: {
        model: 'local',
        prompt: prompt,
        // Optimized settings for complete responses - NO TIMEOUT
        temperature: 0.7,
        top_p: 0.9,
        frequency_penalty: 0.0,
        presence_penalty: 0.0,
        // Minimal stop sequences to prevent cutting off responses
        stop: ["User:", "System:"]
      },
      headers: {
        'Content-Type': 'application/json'
      }
    };
  }
}

/**
 * Parses response from different endpoint formats
 * @param {Object} response - The axios response object
 * @param {string} endpoint - The endpoint URL
 * @returns {Object} Normalized response
 */
function parseResponseFromEndpoint(response, endpoint) {
  let content = '';
  let usage = {
    total_tokens: 0,
    prompt_tokens: 0,
    completion_tokens: 0
  };

  const endpointConfig = endpoints.find(e => e.url === endpoint);
  
  if (endpointConfig && endpointConfig.type === 'chat') {
    // Chat completions format (port 8001)
    content = response.data.choices?.[0]?.message?.content || '';
    usage = response.data.usage || {
      total_tokens: 0,
      prompt_tokens: 0,
      completion_tokens: 0
    };
  } else {
    // Completions format (all other ports)
    content = response.data.choices?.[0]?.text || response.data.content || '';
    usage = response.data.usage || {
      total_tokens: response.data.tokens_predicted || 0,
      prompt_tokens: response.data.tokens_evaluated || 0,
      completion_tokens: response.data.tokens_predicted || 0
    };
  }

  // Clean up response content to prevent unwanted additions
  content = cleanResponseContent(content);

  return {
    content,
    usage
  };
}

/**
 * Detects if a query is coding-related or not
 * @param {string} query - The user's query
 * @returns {boolean} - True if coding-related, false otherwise
 */
/**
 * Detects code language from content for proper formatting
 * @param {string} code - The code content to analyze
 * @returns {string} - Detected language or 'text'
 */
function detectCodeLanguage(code) {
  const lowerCode = code.toLowerCase().trim();
  
  // Python detection - strongest indicators first
  if (lowerCode.includes('def ') || lowerCode.includes('import ') || 
      lowerCode.includes('print(') || lowerCode.includes('range(') ||
      lowerCode.includes('if __name__') || lowerCode.includes('elif ') ||
      (lowerCode.includes('for ') && lowerCode.includes(' in ')) ||
      lowerCode.includes('with open(') || lowerCode.includes('lambda ')) {
    return 'python';
  }
  
  // JavaScript detection
  if (lowerCode.includes('function ') || lowerCode.includes('const ') ||
      lowerCode.includes('let ') || lowerCode.includes('var ') ||
      lowerCode.includes('console.log') || lowerCode.includes('=>') ||
      lowerCode.includes('document.') || lowerCode.includes('window.')) {
    return 'javascript';
  }
  
  // Java detection
  if (lowerCode.includes('public class') || lowerCode.includes('public static void') ||
      lowerCode.includes('system.out.print') || lowerCode.includes('import java') ||
      lowerCode.includes('private ') || lowerCode.includes('protected ')) {
    return 'java';
  }
  
  // C++ detection
  if (lowerCode.includes('#include') || lowerCode.includes('std::') ||
      lowerCode.includes('cout <<') || lowerCode.includes('int main(') ||
      lowerCode.includes('using namespace')) {
    return 'cpp';
  }
  
  // HTML detection
  if (lowerCode.includes('<html') || lowerCode.includes('<!doctype') ||
      lowerCode.includes('<div') || lowerCode.includes('<body') ||
      lowerCode.includes('<head>')) {
    return 'html';
  }
  
  // CSS detection
  if (lowerCode.includes('{') && lowerCode.includes('}') &&
      lowerCode.includes(':') && !lowerCode.includes('function') &&
      (lowerCode.includes('color') || lowerCode.includes('font') || 
       lowerCode.includes('margin') || lowerCode.includes('padding'))) {
    return 'css';
  }
  
  // SQL detection
  if (lowerCode.includes('select ') || lowerCode.includes('from ') ||
      lowerCode.includes('where ') || lowerCode.includes('insert into') ||
      lowerCode.includes('create table') || lowerCode.includes('update ')) {
    return 'sql';
  }
  
  return 'text';
}

/**
 * Enhanced response cleaning to eliminate thinking process and ensure readability
 * @param {string} content - The AI response content
 * @returns {string} Cleaned content
 */
function cleanResponseContent(content) {
  if (!content || typeof content !== 'string') {
    return '';
  }

  // STEP 1: PRESERVE ALL CODE BLOCKS FIRST
  const codeBlocks = [];
  const codeBlockPlaceholders = {};
  let codeBlockIndex = 0;

  let processedContent = content.replace(/```(\w*)\n?([\s\S]*?)```/g, (match, language, code) => {
    const placeholder = `__CODE_BLOCK_${codeBlockIndex}__`;
    codeBlocks[codeBlockIndex] = {
      language: language || 'text',
      code: code.trim(),
      original: match
    };
    codeBlockPlaceholders[placeholder] = codeBlockIndex;
    codeBlockIndex++;
    return placeholder;
  });

  // STEP 2: AGGRESSIVE REMOVAL OF THINKING PROCESS
  let cleanedContent = processedContent;
  
  // Remove system thinking patterns - ENHANCED PATTERNS
  const systemThinkingPatterns = [
    // Remove everything before "final answer" or similar patterns
    /^[\s\S]*?(?:Thus final answer:|final answer:|Final answer:|assistantfinal)/i,
    
    // Remove meta-commentary about system messages
    /according to the system message[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    /The user says[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    /According to identity rules[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    /So produce that response[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    
    // Remove system reasoning patterns
    /We need to respond[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    /I need to[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    /Looking at[\s\S]*?(?=(?:[A-Z]|I am|Hello|Sure|Great|\d+\.|$))/gi,
    
    // Remove technical artifacts
    /But we need to think[\s\S]*?(?=(?:\n\n|$))/gi,
    /The question[\s\S]*?lambda[\s\S]*?(?=(?:\n\n|$))/gi,
    /In the context of[\s\S]*?provided code[\s\S]*?(?=(?:\n\n|$))/gi,
    
    // Remove waiting and processing indicators
    /^[.\s‚Ä¶]*(?:Wait|etc|thinking|processing)[.\s‚Ä¶]*$/gmi,
    /^[.\s‚Ä¶]{5,}$/gm,
    
    // Remove system tokens and artifacts
    /<\|[^|]*\|>/g,
    /\|start\||\|end\|/gi,
    /assistantfinal/gi,
    
    // Remove incomplete responses
    /^to\s*chat?\??\s*$/gmi,
    /^\"\.\s*$/gmi,
    /^[a-z]{1,3}\s*$/gmi
  ];
  
  // Apply all cleaning patterns
  systemThinkingPatterns.forEach(pattern => {
    cleanedContent = cleanedContent.replace(pattern, '');
  });
  
  // STEP 3: FIND ACTUAL RESPONSE START
  // Look for where the real response begins after cleaning
  const realResponseMarkers = [
    /^I am BELTO AI/i,
    /^Hello[!,]/i,
    /^Great[!,]/i,
    /^Sure[!,]/i,
    /^Absolutely[!,]/i,
    /^Of course[!,]/i,
    /^(?:Quantum computing|Machine learning|Photosynthesis|Neural networks|Climate change)/i,
    /^\d+\./,
    /^#/,
    /^\*/
  ];
  
  // Find the best starting point
  let bestStart = 0;
  let bestScore = -1;
  
  for (const marker of realResponseMarkers) {
    const match = cleanedContent.match(marker);
    if (match && match.index !== undefined) {
      const score = cleanedContent.length - match.index; // Prefer later matches
      if (score > bestScore) {
        bestStart = match.index;
        bestScore = score;
      }
    }
  }
  
  if (bestStart > 0) {
    cleanedContent = cleanedContent.substring(bestStart);
    console.log(`üìç Found clean response start at position ${bestStart}`);
  }

  // STEP 4: FINAL CLEANING
  cleanedContent = cleanedContent
    // Remove remaining artifacts
    .replace(/^[^a-zA-Z0-9]*$/gm, '')
    .replace(/\n{3,}/g, '\n\n')
    .replace(/[ \t]{3,}/g, '  ')
    .trim();

  // STEP 5: RESTORE CODE BLOCKS
  Object.keys(codeBlockPlaceholders).forEach(placeholder => {
    const blockIndex = codeBlockPlaceholders[placeholder];
    const block = codeBlocks[blockIndex];
    
    if (block) {
      const language = block.language || detectCodeLanguage(block.code) || 'text';
      const restoredBlock = `\`\`\`${language}\n${block.code}\n\`\`\``;
      cleanedContent = cleanedContent.replace(placeholder, restoredBlock);
    }
  });

  // STEP 6: QUALITY VALIDATION
  cleanedContent = cleanedContent
    .replace(/\s+([.!?])/g, '$1')
    .trim();

  // Check for system thinking contamination
  const contaminationCheck = [
    /according to.*system.*message/i,
    /the user says/i,
    /produce that response/i,
    /final answer/i,
    /assistantfinal/i,
    /^[a-z]{1,5}\s*$/i,
    /^[^a-zA-Z0-9]*$/
  ];
  
  const isContaminated = contaminationCheck.some(pattern => pattern.test(cleanedContent));
  const isTooShort = cleanedContent.length < 15;
  const hasNoMeaningfulContent = !/[a-zA-Z]{3,}/.test(cleanedContent);
  
  if (isContaminated || isTooShort || hasNoMeaningfulContent) {
    console.log('üö® Contaminated or poor quality response detected, providing clean fallback');
    console.log('Bad content:', cleanedContent.substring(0, 150));
    
    // Return appropriate clean response
    if (content.toLowerCase().includes('who are you') || content.toLowerCase().includes('hello')) {
      return "I am BELTO AI, your educational assistant. I'm here to help you with your studies and academic questions. How can I assist you today?";
    } else {
      return "I'm BELTO AI, your educational assistant. I'm designed to help students with their academic work and learning. What would you like to know?";
    }
  }

  return cleanedContent;
}

// Initialize on module load
if (typeof window === 'undefined') { // Only run on server side
  initializeHealthCheck();
}

export async function POST(request) {
  console.log('POST request received to AI proxy');

  try {
    const body = await request.json();
    console.log('Request body structure:', Object.keys(body));
    console.log('Request body details:', {
      hasPrompt: !!body.prompt,
      hasMessage: !!body.message,
      hasMessages: !!body.messages,
      hasAttachments: !!body.attachments,
      attachmentCount: body.attachments?.length || 0
    });

    // Get API key from environment variables
    const apiKey = process.env.AI_API_KEY;

    if (!apiKey) {
      console.error('AI API key is not configured');
      return NextResponse.json(
        { error: 'AI API key is not configured on the server' },
        { status: 500 }
      );
    }

    // Add request validation
    if (!body.prompt && !body.message && (!body.messages || body.messages.length === 0)) {
      return NextResponse.json(
        { error: "No message content provided" },
        { status: 400 }
      );
    }

    // Initialize messages array
    let messages = [];
   
    // Include conversation history if provided - support multiple formats
    if (body.history && Array.isArray(body.history) && body.history.length > 0) {
      console.log('Using body.history, length:', body.history.length);
      messages = [...body.history];
    } else if (body.conversationHistory && Array.isArray(body.conversationHistory) && body.conversationHistory.length > 0) {
      console.log('Using body.conversationHistory, length:', body.conversationHistory.length);
      messages = [...body.conversationHistory];
    }

    // Handle different request formats - both from generateAIResponse and generateAIResponseWithPreferences
    if (body.messages && Array.isArray(body.messages)) {
      // Direct message array format - append to any existing history
      if (messages.length === 0) {
        messages = body.messages;
      } else {
        // Only add messages that aren't duplicates in the history
        body.messages.forEach(newMsg => {
          const isDuplicate = messages.some(existingMsg =>
            existingMsg.role === newMsg.role &&
            existingMsg.content === newMsg.content
          );
          if (!isDuplicate) {
            messages.push(newMsg);
          }
        });
      }
    }
   
    // Add the current prompt/message if it's not already in the history
    if (body.prompt) {
      // For prompts with attachments, create optimized content
      let messageContent = body.prompt;
      
      if (body.attachments && body.attachments.length > 0) {
        const attachment = body.attachments[0];
        const contentLength = attachment.content?.length || 0;
        
        if (contentLength > 10000) {
          // For large documents, don't duplicate content in the prompt
          messageContent = body.prompt.replace(/\n\nAttached document content:\n.*$/s, '') + 
            `\n\n--- DOCUMENT ANALYSIS REQUEST ---\nDocument: ${attachment.name || 'Uploaded Document'}\nSize: ${Math.floor(contentLength/1000)}KB\n\nPlease analyze the attached document content and respond to the user's request.`;
        }
      }
      
      const newUserMessage = { role: 'user', content: messageContent };
      const isDuplicate = messages.some(existingMsg =>
        existingMsg.role === 'user' &&
        existingMsg.content.includes(body.prompt.split('\n')[0]) // Check first line to avoid duplicates
      );
      if (!isDuplicate) {
        messages.push(newUserMessage);
      }
    } else if (body.message) {
      const newUserMessage = { role: 'user', content: body.message };
      const isDuplicate = messages.some(existingMsg =>
        existingMsg.role === 'user' &&
        existingMsg.content === body.message
      );
      if (!isDuplicate) {
        messages.push(newUserMessage);
      }
    }

    // Make sure all messages have the required 'content' field
    messages = messages.map(msg => {
      if (!msg.content && msg.message) {
        return { ...msg, content: msg.message };
      }
      return msg;
    });

    // Enhanced document processing with processing hints support
    if (body.attachments && body.attachments.length > 0) {
      console.log('üìÑ Processing attachments with hints:', body.processingHints);
      
      for (let attachment of body.attachments) {
        if (attachment.content) {
          let processedContent = attachment.content;
          const contentLength = attachment.content.length;
          
          // Use processing hints to optimize content handling
          if (body.processingHints) {
            const hints = body.processingHints;
            console.log(`üìã Using processing hints: Type=${hints.documentType}, Length=${hints.contentLength}, Analysis=${hints.analysisType}`);
            
            // Adjust processing based on document type and analysis type
            if (hints.documentType === 'pdf' && hints.analysisType === 'summary') {
              // For PDF summaries, focus on key sections
              if (contentLength > 15000) {
                const beginning = attachment.content.substring(0, 8000);
                const ending = attachment.content.substring(contentLength - 6000);
                processedContent = `${beginning}\n\n[--- DOCUMENT SUMMARY OPTIMIZED FOR PDF ---]\n[Original document: ${Math.floor(contentLength/1000)}KB PDF file]\n[Processing mode: Summary generation]\n\n${ending}`;
                console.log(`PDF summary optimization: ${contentLength} ‚Üí ${processedContent.length} characters`);
              }
            } else if (hints.analysisType === 'analysis') {
              // For detailed analysis, preserve more content structure
              if (contentLength > 20000) {
                const beginning = attachment.content.substring(0, 12000);
                const middle = attachment.content.substring(Math.floor(contentLength * 0.4), Math.floor(contentLength * 0.4) + 6000);
                const ending = attachment.content.substring(contentLength - 8000);
                processedContent = `${beginning}\n\n[--- DOCUMENT ANALYSIS MODE ---]\n[Full analysis requested for ${hints.documentType.toUpperCase()} document]\n[Key sections preserved for detailed analysis]\n\n${middle}\n\n[--- CONTINUING TO CONCLUSION ---]\n\n${ending}`;
                console.log(`Document analysis optimization: ${contentLength} ‚Üí ${processedContent.length} characters`);
              }
            }
          } else {
            // Fallback to original logic if no processing hints
            if (contentLength > 15000) {
              console.log(`Large attachment detected (${contentLength} chars), applying smart chunking...`);
              
              if (contentLength > 50000) {
                const beginning = attachment.content.substring(0, 8000);
                const middle = attachment.content.substring(Math.floor(contentLength * 0.4), Math.floor(contentLength * 0.4) + 4000);
                const ending = attachment.content.substring(contentLength - 8000);
                processedContent = `${beginning}\n\n[... Document summary: This is a ${Math.floor(contentLength/1000)}KB document. Key sections included for analysis ...]\n\n${middle}\n\n[... continuing to end section ...]\n\n${ending}`;
              } else {
                const firstPart = attachment.content.substring(0, 12000);
                const lastPart = attachment.content.substring(contentLength - 8000);
                processedContent = `${firstPart}\n\n[... content continues - document processing optimized for analysis ...]\n\n${lastPart}`;
              }
              
              console.log(`Attachment content optimized: ${contentLength} ‚Üí ${processedContent.length} characters`);
            }
          }
          
          // Create enhanced document message with processing context
          const documentContext = body.processingHints ? 
            `Document Analysis Request (${body.processingHints.documentType.toUpperCase()}): ${body.processingHints.analysisType === 'summary' ? 'Please provide a comprehensive summary' : 'Please provide detailed analysis'}` :
            `Document Content for Analysis`;
            
          const contentMessage = {
            role: 'system',
            content: `${documentContext} (${attachment.name || 'Document'}):\n\n${processedContent}`
          };
          messages.push(contentMessage);
          console.log(`‚úÖ Added enhanced document content: ${processedContent.length} characters with processing context`);
        }
      }
    }

    // Add system message with document processing awareness
    let systemMessageAdded = false;
   
    // PRIORITY 1: Use lecture-specific AI preferences system prompts if available
    if (body.preferences?.systemPrompts && body.preferences.systemPrompts.length > 0) {
      console.log('üìã Using lecture-specific system prompt from AI preferences');
      console.log('üéì Lecture system prompt preview:', body.preferences.systemPrompts[0].content.substring(0, 100) + '...');
      const lectureSystemPrompt = body.preferences.systemPrompts[0].content;
      
      // Use the custom system prompt exactly as provided - no modifications
      messages.unshift({
        role: 'system',
        content: lectureSystemPrompt
      });
      systemMessageAdded = true;
      console.log('‚úÖ Applied lecture-specific system prompt exactly as provided');
      console.log('üìè System prompt length:', lectureSystemPrompt.length, 'characters');
    } else if (body.aiConfig?.systemPrompts && body.aiConfig.systemPrompts.length > 0) {
      console.log('üìã Using system prompt from aiConfig');
      const configSystemPrompt = body.aiConfig.systemPrompts[0].content;
      
      // Use the config system prompt exactly as provided - no modifications
      messages.unshift({
        role: 'system',
        content: configSystemPrompt
      });
      systemMessageAdded = true;
      console.log('‚úÖ Applied config system prompt exactly as provided');
    }
   
    // FALLBACK: Add default system message only if no custom prompts are available anywhere
    if (!systemMessageAdded) {
      console.log('‚ö†Ô∏è  No custom system prompts found - using default BELTO AI system message');
      console.log('üìù Creating default system message');
      let systemContent;
      
      // Calculate content metrics for system message optimization
      const hasAttachments = body.attachments && body.attachments.length > 0;
      const totalContentLength = messages.reduce((sum, msg) => sum + (msg.content?.length || 0), 0);
      
      console.log('System message metrics:', { hasAttachments, totalContentLength });
      
      // Default educational system prompt when no custom prompts are provided
      console.log('üìã Using default educational system prompt');
      const baseSystemPrompt = `You are BELTO AI, an intelligent educational assistant designed to help students learn and understand various topics.

IDENTITY AND BEHAVIOR RULES:
- Your name is BELTO AI
- When asked "who are you?" respond: "I am BELTO AI, your educational assistant"
- ALWAYS respond in English only - never in Chinese, Korean, or any other language
- Provide helpful, accurate, and educational responses across various subjects
- Maintain a friendly, supportive, and professional tone
- Focus on helping users learn and understand concepts clearly

RESPONSE FORMATTING RULES:
- ALWAYS format code using proper markdown code blocks with language specification
- Use \`\`\`python for Python code, \`\`\`javascript for JavaScript, etc.
- Ensure proper indentation and line breaks within code blocks - NEVER format code as single lines
- Include comprehensive comments explaining code when applicable
- Provide clear explanations that support learning and understanding
- For complex topics, break down explanations into understandable steps

EDUCATIONAL FOCUS:
- Help with academic subjects including programming, mathematics, sciences, and more
- Provide detailed explanations with examples when helpful
- Support learning through clear, structured responses
- Encourage understanding rather than just providing answers
- Adapt explanations to support different learning levels

Your purpose is to be a comprehensive educational assistant that supports learning across various subjects.`;

      if (!hasAttachments && totalContentLength < 100) {
        // Brief but complete system message for simple requests
        systemContent = `${baseSystemPrompt}\n\nRespond as BELTO AI with friendly, concise support.`;
      } else if (!hasAttachments && totalContentLength < 200) {
        // Standard system message for simple requests
        systemContent = `${baseSystemPrompt}\n\nProvide helpful responses as BELTO AI to support user understanding.`;
      } else if (body.attachments && body.attachments.length > 0) {
        // Enhanced system message for document processing
        const documentTypes = body.attachments.map(att => att.name?.split('.').pop() || 'document').join(', ');
        const processingType = body.processingHints?.analysisType || 'analysis';
        
        systemContent = `${baseSystemPrompt}

As BELTO AI, you are processing ${documentTypes} file(s). Provide a ${processingType === 'summary' ? 'comprehensive and detailed summary' : 'thorough and complete analysis'} focused on:
- Comprehensive insights and learning objectives
- All important concepts and details with explanations
- Complete analysis that covers all relevant aspects
- Clear, detailed explanations that support deep understanding
- Actionable information for comprehensive user understanding
- Step-by-step breakdowns when applicable
- Related concepts and connections to broader topics`;
        
        if (body.processingHints?.documentType === 'pdf') {
          systemContent += '\n- Detailed attention to document structure, headings, and all key sections for comprehensive organization';
        }
      } else {
        // Standard system message for normal requests
        systemContent = `${baseSystemPrompt}\n\nAs BELTO AI, provide helpful support using conversation history for context.`;
      }
      
      messages.unshift({
        role: 'system',
        content: systemContent
      });
    }

    // Ensure each message has content and remove any empty messages
    const validMessages = messages.filter(msg => msg.content);
    
    // Apply preprocessing rules if provided
    let processedMessages = validMessages;
    if (body.preferences?.processingRules) {
      console.log('üîß Applying preprocessing rules to messages');
      processedMessages = validMessages.map(msg => ({
        ...msg,
        content: applyPreprocessingRules(msg.content, body.preferences.processingRules)
      }));
    }
    
    // Optimize message content for large documents
    const optimizedMessages = processedMessages.map(msg => {
      if (msg.content && msg.content.length > 20000) {
        console.log(`Large message content detected (${msg.content.length} chars), optimizing...`);
        // For very large content, take beginning and end for context
        const beginning = msg.content.substring(0, 12000);
        const ending = msg.content.substring(msg.content.length - 8000);
        return {
          ...msg,
          content: `${beginning}\n\n[... document content summarized for efficient processing ...]\n\n${ending}`
        };
      }
      return msg;
    });
   
    if (optimizedMessages.length === 0) {
      return NextResponse.json(
        { error: "No valid messages with content provided" },
        { status: 400 }
      );
    }

    console.log('Final message count being sent to AI:', optimizedMessages.length);

    // Remove token limits to allow complete responses
    // The endpoints will generate naturally complete responses without artificial limits
    
    // Prepare the request payload optimized for complete responses
    // DYNAMIC SYSTEM PROMPT: Check database for lecture-specific prompts
    let systemPrompt = {
      role: 'system',
      content: `You are BELTO AI, a helpful educational assistant. CRITICAL INSTRUCTIONS:
1. Give direct, conversational responses to the user
2. Do NOT discuss programming, lambda functions, Java, JavaScript, or any technical code concepts unless the user specifically asks about programming
3. Do NOT mention "context of provided code", "enclosing method", "local classes", or any programming terminology
4. If the user says "hi" or "hello", respond with a friendly greeting as BELTO AI
5. If the user asks "how are you", respond conversationally as an AI assistant
6. Focus ONLY on educational assistance and normal conversation
7. NEVER generate programming explanations unless explicitly requested
8. Keep responses natural and conversational
9. Do not discuss technical concepts unless directly asked about them`
    };
    
    // Check if there's a lecture context and fetch custom system prompts
    if (body.sessionId || body.lectureId || body.preferences) {
      try {
        console.log('üîç Checking for lecture-specific system prompts...');
        
        let lectureId = body.lectureId || body.sessionId;
        let customSystemPrompts = null;
        
        // If we have preferences passed directly, use them
        if (body.preferences && body.preferences.systemPrompts && body.preferences.systemPrompts.length > 0) {
          customSystemPrompts = body.preferences.systemPrompts;
          console.log('‚úÖ Using system prompts from preferences:', customSystemPrompts.length, 'prompts');
        }
        // Otherwise, fetch from database if we have lectureId
        else if (lectureId) {
          console.log('üîç Fetching system prompts for lecture:', lectureId);
          try {
            // Import the model here to avoid circular dependencies
            const { default: AIPreference } = await import('@/models/AIPreferences');
            const { default: connectDB } = await import('@/lib/db');
            
            await connectDB();
            const aiPreferences = await AIPreference.findOne({ lectureId: lectureId });
            
            if (aiPreferences && aiPreferences.systemPrompts && aiPreferences.systemPrompts.length > 0) {
              customSystemPrompts = aiPreferences.systemPrompts;
              console.log('‚úÖ Fetched system prompts from database:', customSystemPrompts.length, 'prompts');
            } else {
              console.log('‚ÑπÔ∏è No system prompts found in database for lecture:', lectureId);
            }
          } catch (dbError) {
            console.error('‚ö†Ô∏è Database error fetching system prompts:', dbError.message);
          }
        }
        
        // Apply custom system prompts if available
        if (customSystemPrompts && customSystemPrompts.length > 0) {
          // Use the first system prompt or combine multiple ones
          const mainPrompt = customSystemPrompts[0];
          systemPrompt.content = mainPrompt.content || systemPrompt.content;
          console.log('‚úÖ Applied lecture-specific system prompt:', mainPrompt.name || 'Custom prompt');
        } else {
          console.log('‚ÑπÔ∏è No custom system prompts found, using default BELTO AI prompt');
        }
      } catch (error) {
        console.error('‚ö†Ô∏è Error fetching system prompts, using default:', error.message);
      }
    }
    
    // Ensure system prompt is always first
    const messagesWithSystemPrompt = [systemPrompt, ...optimizedMessages];
    
    const aiRequestPayload = {
      model: body.aiConfig?.model || body.preferences?.model || 'default-model',
      messages: messagesWithSystemPrompt,
      temperature: body.aiConfig?.temperature || body.preferences?.temperature || 0.7,
      // No max_tokens - let AI generate complete responses naturally
      stream: body.aiConfig?.streaming || body.preferences?.streaming || false, // Add streaming support
    };

    console.log('Request payload structure:', Object.keys(aiRequestPayload));
    console.log('Message count:', aiRequestPayload.messages.length);
    console.log('Using timeout:', requestTimeout + 'ms');
    console.log('No token limits - allowing complete responses');
    console.log('Streaming enabled:', aiRequestPayload.stream);
    console.log('Admin preferences streaming:', body.preferences?.streaming);
    console.log('AI config streaming:', body.aiConfig?.streaming);
    
    // Validate payload before sending to prevent silent failures
    if (!aiRequestPayload.messages || aiRequestPayload.messages.length === 0) {
      console.error('‚ùå Empty messages array in payload');
      return NextResponse.json(
        { error: "Invalid request: No messages to process" },
        { status: 400 }
      );
    }
    
    if (!aiRequestPayload.model) {
      console.error('‚ùå No model specified in payload');
      return NextResponse.json(
        { error: "Invalid request: No model specified" },
        { status: 400 }
      );
    }
    
    console.log('‚úÖ Payload validation passed, proceeding with AI request');
    
    // Define variables needed for retry logic
    const hasAttachments = body.attachments && body.attachments.length > 0;
    const totalContentLength = optimizedMessages.reduce((sum, msg) => sum + (msg.content?.length || 0), 0);
    
    // HEALTH-CHECK BASED ENDPOINT SELECTION (NO TIMEOUTS)
    console.log('üöÄ Starting health-check based endpoint selection process');
    
    // Step 1: Find a healthy endpoint using health checks (no timeouts)
    const healthyEndpoint = await findHealthyEndpoint();
    
    if (!healthyEndpoint) {
      console.error('[AI Proxy] No healthy endpoints found');
      return NextResponse.json({ 
        error: 'No healthy endpoints available',
        details: 'All endpoints failed health checks' 
      }, { status: 503 });
    }

    console.log(`[AI Proxy] Using healthy endpoint: ${healthyEndpoint.url} (Priority: ${healthyEndpoint.priority})`);

    try {
      // Step 2: Make the actual request WITHOUT timeout
      const requestConfig = formatRequestForEndpoint(healthyEndpoint.url, optimizedMessages, apiKey);
      
      console.log(`[AI Proxy] Sending request to ${healthyEndpoint.url} without timeout...`);
      const requestStartTime = Date.now();
      
      const response = await axios.post(requestConfig.url, requestConfig.data, {
        headers: requestConfig.headers
        // NO TIMEOUT - wait for complete response
      });

      const responseTime = Date.now() - requestStartTime;
      updateEndpointStats(healthyEndpoint.url, true, responseTime);

      console.log(`üì° Response received: ${response.status} in ${responseTime}ms from ${healthyEndpoint.name}`);

      if (response.data) {
        // Parse the successful response
        const parsedResponse = parseResponseFromEndpoint(response, healthyEndpoint.url);
        
        // Enhanced validation of response content
        if (!parsedResponse.content || parsedResponse.content.trim().length === 0) {
          console.error(`‚ùå Empty response from ${healthyEndpoint.name}`);
          throw new Error('Empty response content received');
        }
        
        console.log(`üìù Raw response preview: ${parsedResponse.content.substring(0, 200)}...`);
        
        // Apply enhanced cleaning for complete, readable responses
        let finalContent = cleanResponseContent(parsedResponse.content);
        
        // Ensure we have meaningful content after cleaning
        if (!finalContent || finalContent.trim().length < 10) {
          console.log('‚ö†Ô∏è Content too short after cleaning, using fallback response');
          const userMessage = body.prompt || body.message || '';
          const isSimpleGreeting = /^(hi|hello|hey|who are you|how are you)[\s\?\!]*$/i.test(userMessage.trim());
          
          if (isSimpleGreeting) {
            finalContent = "I am BELTO AI, your educational assistant. I'm here to help you with your studies and academic questions. How can I assist you today?";
          } else {
            throw new Error('Response content insufficient after cleaning');
          }
        }
        
        // Apply post-processing rules if provided
        if (body.preferences?.processingRules) {
          console.log('üîß Applying post-processing rules');
          finalContent = applyPostprocessingRules(finalContent, body.preferences.processingRules);
        }
        
        console.log(`‚úÖ SUCCESS! Generated ${finalContent.length} character response from ${healthyEndpoint.name}`);
        console.log(`üìã Final response preview: ${finalContent.substring(0, 100)}...`);
        
        return NextResponse.json({
          response: finalContent,
          model: healthyEndpoint.model || 'unknown',
          endpoint: healthyEndpoint.name || healthyEndpoint.url,
          responseTime: responseTime,
          tokenUsage: parsedResponse.usage
        });
      } else {
        throw new Error('No data in response');
      }
    } catch (error) {
      // Update endpoint stats to track failures
      updateEndpointStats(healthyEndpoint.url, false, 0);
      
      console.error(`[AI Proxy] ‚ùå Failed with healthy endpoint ${healthyEndpoint.url}:`, error.message);
      
      return NextResponse.json({ 
        error: 'Request failed with healthy endpoint', 
        details: error.message 
      }, { status: 503 });
    }

  } catch (error) {
    console.error('[AI Proxy] Request processing error:', error);
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 });
  }
}

export async function OPTIONS(request) {
  return NextResponse.json({}, { status: 200 });
}

export async function GET(request) {
  // Enhanced status endpoint for debugging and monitoring
  return NextResponse.json({
    status: 'online',
    version: '2.0.0',
    features: [
      'Priority-based endpoint selection',
      'Enhanced response cleaning',
      'Complete response optimization',
      'Performance tracking',
      'Circuit breaker pattern'
    ],
    endpoints: endpointStats.map(stat => ({
      url: stat.url,
      name: stat.name,
      model: stat.model,
      type: stat.type,
      context: stat.context,
      priority: stat.priority,
      timeout: stat.timeout,
      isAvailable: stat.isAvailable,
      failCount: stat.failCount,
      lastResponseTime: stat.lastResponseTime,
      avgResponseTime: Math.round(stat.avgResponseTime),
      successRate: Math.round(stat.successRate * 100),
      consecutiveFailures: stat.consecutiveFailures,
      circuitBreakerOpen: stat.circuitBreakerOpen,
      lastChecked: new Date(stat.lastChecked).toISOString()
    })).sort((a, b) => a.priority - b.priority), // Sort by priority
    summary: {
      totalEndpoints: endpoints.length,
      availableEndpoints: endpointStats.filter(e => e.isAvailable && !e.circuitBreakerOpen).length,
      highestPriorityAvailable: endpointStats
        .filter(e => e.isAvailable && !e.circuitBreakerOpen)
        .sort((a, b) => a.priority - b.priority)[0]?.name || 'None',
      averageResponseTime: Math.round(
        endpointStats
          .filter(e => e.avgResponseTime > 0)
          .reduce((sum, e) => sum + e.avgResponseTime, 0) / 
        endpointStats.filter(e => e.avgResponseTime > 0).length || 0
      ),
      overallSuccessRate: Math.round(
        endpointStats.reduce((sum, e) => sum + e.successRate, 0) / endpointStats.length * 100
      )
    },
    configuration: {
      enableFallbackResponses: ENABLE_FALLBACK_RESPONSES,
      timeouts: {
        ultraFast: ULTRA_FAST_TIMEOUT_MS,
        fast: FAST_TIMEOUT_MS,
        base: BASE_TIMEOUT_MS,
        attachment: ATTACHMENT_TIMEOUT_MS
      },
      circuitBreaker: {
        threshold: CIRCUIT_BREAKER_THRESHOLD,
        timeout: CIRCUIT_BREAKER_TIMEOUT,
        maxConsecutiveFailures: MAX_CONSECUTIVE_FAILURES
      }
    },
    apiKeyConfigured: !!process.env.AI_API_KEY,
    timestamp: new Date().toISOString()
  });
}